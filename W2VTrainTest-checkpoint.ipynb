{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# languange processing imports\n",
    "import nltk\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# model imports\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set()  # defines the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import operator\n",
    "from tqdm import tqdm_notebook as tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of function to preprocess comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and remove non-ascii words\n",
    "# I stored our special word in a variable for later use\n",
    "\n",
    "our_special_word = 'qwerty'\n",
    "\n",
    "def remove_ascii_words(df):\n",
    "    \"\"\" removes non-ascii characters from the 'texts' column in df.\n",
    "    It returns the words containig non-ascii characers.\n",
    "    \"\"\"\n",
    "    non_ascii_words = []\n",
    "    for i in range(len(df)):\n",
    "        for word in df.loc[i, 'body'].split(' '):\n",
    "            if any([ord(character) >= 128 for character in word]):\n",
    "                non_ascii_words.append(word)\n",
    "                df.loc[i, 'body'] = df.loc[i, 'body'].replace(word, our_special_word)\n",
    "    return non_ascii_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_tokens(sentence):\n",
    "    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), sentence))\n",
    "    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n",
    "    return removed_punctation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get transform the documents into sentences for the word2vecmodel\n",
    "# we made a function such that later on when we make the submission, we don't need to write duplicate code\n",
    "def w2v_preprocessing(df):\n",
    "    \"\"\" All the preprocessing steps for word2vec are done in this function.\n",
    "    All mutations are done on the dataframe itself. So this function returns\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "    df['body'] = df.body.str.lower()\n",
    "    df['document_sentences'] = df.body.str.split('.')  # split texts into individual sentences\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(map(nltk.word_tokenize, sentences)),\n",
    "                                         df.document_sentences))  # tokenize sentences\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(map(get_good_tokens, sentences)),\n",
    "                                         df.tokenized_sentences))  # remove unwanted characters\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(filter(lambda lst: lst, sentences)),\n",
    "                                         df.tokenized_sentences))  # remove empty lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess train_data and test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv.gz\", compression=\"gzip\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ascii_words = remove_ascii_words(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 773082.\n",
      "Number of texts: 296042.\n"
     ]
    }
   ],
   "source": [
    "#create dictionary with all sentences\n",
    "sentences = []\n",
    "for sentence_group in train_data.tokenized_sentences:\n",
    "    sentences.extend(sentence_group)\n",
    "\n",
    "print(\"Number of sentences: {}.\".format(len(sentences)))\n",
    "print(\"Number of texts: {}.\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107946, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ejchristian86</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I hadn't ever heard of them before joining thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZenDragon</td>\n",
       "      <td>gaming</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>At 7680 by 4320 with 64x AA, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>savoytruffle</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>bite me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author        subreddit   created_utc  \\\n",
       "0  ejchristian86  TwoXChromosomes  1.388534e+09   \n",
       "1      ZenDragon           gaming  1.388534e+09   \n",
       "2   savoytruffle        AskReddit  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I hadn't ever heard of them before joining thi...  \n",
       "1                At 7680 by 4320 with 64x AA, right?  \n",
       "2                                            bite me  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test_data.csv.gz\", compression=\"gzip\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2710101.\n",
      "Number of texts: 1107056.\n"
     ]
    }
   ],
   "source": [
    "sentences2 = []\n",
    "for sentence_group in test_data.tokenized_sentences:\n",
    "    sentences2.extend(sentence_group)\n",
    "\n",
    "print(\"Number of sentences: {}.\".format(len(sentences2)))\n",
    "print(\"Number of texts: {}.\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using both train and test data to train w2v\n",
    "sentences = sentences + sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality\n",
    "min_word_count = 3    # Minimum word count\n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "# this model is a two-layer neural networks that are trained to reconstruct linguistic contexts of words.\n",
    "W2Vmodel = Word2Vec(sentences=sentences,\n",
    "                    sg=1,        # alleno l'algoritmo con lo skip-gram\n",
    "                    hs=0,        # allenameto fatto con sampling non negativi\n",
    "                    workers=num_workers,\n",
    "                    size=num_features,\n",
    "                    min_count=min_word_count,  #ignoro parole con freq minore dimin_word_count\n",
    "                    window=context,  #range max delle parole da considerare come contesto.\n",
    "                    sample=downsampling,\n",
    "                    negative=5,\n",
    "                    iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_features(w2v_model, sentence_group):\n",
    "    \"\"\" Transform a sentence_group (containing multiple lists\n",
    "    of words) into a feature vector. It averages out all the\n",
    "    word vectors of the sentence_group.\n",
    "    \"\"\"\n",
    "    words = np.concatenate(sentence_group)  # words in text\n",
    "    index2word_set = set(w2v_model.wv.vocab.keys())  # words known to model\n",
    "    \n",
    "    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n",
    "    \n",
    "    # Initialize a counter for number of words in a review\n",
    "    nwords = 0\n",
    "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            featureVec = np.add(featureVec, w2v_model[word])\n",
    "            nwords += 1.\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.body = train_data['body'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 1396 words with characters with an ordinal >= 128 in the test data.\n"
     ]
    }
   ],
   "source": [
    "non_ascii_words = remove_ascii_words(train_data)\n",
    "\n",
    "print(\"Replaced {} words with characters with an ordinal >= 128 in the test data.\".format(\n",
    "    len(non_ascii_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(train_data[train_data.tokenized_sentences.str.len() == 0].index, inplace= True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamesjun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "train_data['w2v_features'] = list(map(lambda sen_group:\n",
    "                                     get_w2v_features(W2Vmodel, sen_group),\n",
    "                                     train_data.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainw2v = train_data.drop(columns = ['subreddit','created_utc','document_sentences', 'tokenized_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainw2v.to_pickle('TrainW2V.csv')  # train_data vettorizzato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vettorizzo anche il test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.body = test_data['body'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 33975 words with characters with an ordinal >= 128 in the test data.\n"
     ]
    }
   ],
   "source": [
    "non_ascii_words = remove_ascii_words(test_data)\n",
    "\n",
    "print(\"Replaced {} words with characters with an ordinal >= 128 in the test data.\".format(\n",
    "    len(non_ascii_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(test_data[test_data.tokenized_sentences.str.len() == 0].index, inplace= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamesjun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "test_data['w2v_features'] = list(map(lambda sen_group:\n",
    "                                     get_w2v_features(W2Vmodel, sen_group),\n",
    "                                     test_data.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "testw2v = test_data.drop(columns = ['subreddit','created_utc','document_sentences', 'tokenized_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "testw2v.to_pickle('TestW2V.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
